{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "comprehensive-turkey",
   "metadata": {},
   "source": [
    "# Análise de texto de fontes desestruturadas e Web\n",
    "\n",
    "## Aula 12\n",
    "\n",
    "Nesta aula veremos como realizar **Análise de Tópicos** utilizando bibliotecas de processamento de linguagem natural. Mais precisamente, utilizaremos técnicas de **topic modeling**, um conjunto de algoritmos não supervisionados, ou seja, diferente do que fizemos nas aulas 10 e 11, quando tínhamos as categorias das notícias e treinávamos modelos de Machine Learning para aprender padrões que separam as classes, agora não temos mais estas categorias de antemão. Assim, algoritmos não supervisionados devem inferir categorias ou tópicos que emergem de forma natural nos dados.\n",
    "\n",
    "A biblioteca utilizada será a **Gensim**.\n",
    "\n",
    "Para conhecer mais sobre ela, acesse https://radimrehurek.com/gensim/index.html e https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff316f9b",
   "metadata": {},
   "source": [
    "## Baixar os dados no Colab\n",
    "\n",
    "Para baixar os dados no colab, utilize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e041d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://atd-insper.s3.us-east-2.amazonaws.com/aula12/noticias1.xlsx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-cable",
   "metadata": {},
   "source": [
    "## Importando as bibliotecas necessárias\n",
    "\n",
    "Agora, vamos importar as bibliotecas necessárias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-letter",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-patrol",
   "metadata": {},
   "source": [
    "## Importando as bibliotecas necessárias\n",
    "\n",
    "Agora, vamos importar as bibliotecas necessárias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-france",
   "metadata": {},
   "outputs": [],
   "source": [
    "# para trabalhar com diretórios / sistema operacional\n",
    "import os\n",
    "\n",
    "# para trabalhar com expressões regulares\n",
    "import re\n",
    "\n",
    "# utilizada para nos indicar o caminho do executável do Python\n",
    "import sys\n",
    "\n",
    "# para pandas DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "# topic Modeling\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# plot\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-regard",
   "metadata": {},
   "source": [
    "Caso obtenha algum erro, utilize o **!pip install** para instalar a biblioteca ausente!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-voluntary",
   "metadata": {},
   "source": [
    "Você pode conferir de onde está executando o Python e qual a versão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-secondary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Executável:')\n",
    "print(sys.executable)\n",
    "\n",
    "print('\\nVersão do Python:')\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-premium",
   "metadata": {},
   "source": [
    "Vamos conferir em qual diretório iremos trabalhar (é o diretório do notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-quantity",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('O seu notebook está na pasta:')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-former",
   "metadata": {},
   "source": [
    "Versão das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-chance",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('gensim {}'.format(gensim.__version__))\n",
    "print('nltk {}'.format(nltk.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-puzzle",
   "metadata": {},
   "source": [
    "# Exemplo\n",
    "\n",
    "Vamos aprender como realizar modelagem de tópicos com o Python. Para isso, vamos criar um DataFrame de exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-caution",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfex = pd.DataFrame({'Texto': ['quero ir para Praia curtir o sol. Praia da barra',\n",
    "                             'não gosto de praia, só de sol',\n",
    "                             'saudade da sombra e água quente da praia',\n",
    "                             'tenho um cão que adora comer e passear',\n",
    "                             'meu cão está doente. Foi algo que ele comeu',\n",
    "                             'cuide do seu cão para que não fique doente após passear']})\n",
    "dfex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-clark",
   "metadata": {},
   "source": [
    "Vamos considerar que cada linha deste DataFrame representa um **documento**. Ou seja, temos seis documentos nesta base de dados.\n",
    "\n",
    "Perceba que o DataFrame possui apenas uma coluna de textos, ou seja, não nenhuma variável que indique categorias. Apesar disso, podemos facilmente perceber que alguns documentos falam sobre **praia** e outros que falam sobre **cachorros**.\n",
    "\n",
    "Para um humano isto é fácil de perceber, agora, veremos como fazer isto no Python!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-symphony",
   "metadata": {},
   "source": [
    "# Pré-processamento\n",
    "\n",
    "Utilizaremos diversas técnicas aprendidas durante o curso para limpar e transformar os textos que queremos analisar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-broadcasting",
   "metadata": {},
   "source": [
    "Vamos converter o texto para **minúsculo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-point",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfex['Texto'] = dfex['Texto'].str.lower()\n",
    "dfex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-recorder",
   "metadata": {},
   "source": [
    "E separar o texto em uma **lista de palavras** (tokenização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-latest",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_texto = dfex['Texto'].apply(word_tokenize)\n",
    "print(token_texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-modification",
   "metadata": {},
   "source": [
    "Em seguida, vamos criar um **corpus** textual, que conterá uma representação dos termos ou palavras presentes em cada documento. Aqui, teremos uma representação **Bag of Words**, semelhante ao visto nas duas últimas aulas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-pencil",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = corpora.Dictionary(token_texto)\n",
    "corpus = [dic.doc2bow(token) for token in token_texto]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f91dc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conferindo a vetorização de uma frase qualquer\n",
    "dic.doc2bow(['o', 'quero', 'quero', 'canta', 'para', 'o', 'cão'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cec9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entendendo os IDs\n",
    "dic.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-showcase",
   "metadata": {},
   "source": [
    "## Latent Dirichlet allocation (LDA)\n",
    "\n",
    "Agora, vamos utilizar a biblioteca Gensim para construir um modelo LDA e identificar tópicos no nosso corpus textual.\n",
    "\n",
    "Considere conferir a documentação do **LdaModel** https://radimrehurek.com/gensim/models/ldamodel.html e seus parâmetros disponíveis.\n",
    "\n",
    "Primeiramente, vamos considerar dois tópicos e uma palavra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-mortgage",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = models.ldamodel.LdaModel(corpus, num_topics=2, id2word=dic, passes=25, random_state=1, iterations=100)\n",
    "\n",
    "lista_topico = lda.print_topics(num_words=1)\n",
    "\n",
    "for topico in lista_topico:\n",
    "    print(topico)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-bench",
   "metadata": {},
   "source": [
    "Agora, vamos tentar dois tópicos e mais palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-application",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = models.ldamodel.LdaModel(corpus, num_topics=2, id2word=dic, passes=5, iterations=100)\n",
    "\n",
    "lista_topico = lda.print_topics(num_words=3)\n",
    "\n",
    "for topico in lista_topico:\n",
    "    print(topico)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-proxy",
   "metadata": {},
   "source": [
    "## Plot dos tópicos encontrados\n",
    "\n",
    "É conveninente ter a possibilidade de visualizar graficamente os tópicos encontrados.\n",
    "\n",
    "Para isso, utilizaremos uma biblioteca chamada **plotly**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-technical",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_top = lda.show_topics(formatted=False)\n",
    "\n",
    "topico = lda_top[0][1]\n",
    "topico.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "palavra = list(zip(*topico))[0]\n",
    "score = list(zip(*topico))[1]\n",
    "\n",
    "px.bar(x=palavra, y=score, labels={'x': 'Palavra', 'y': 'Score'}, title='Tópico 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-coordinate",
   "metadata": {},
   "outputs": [],
   "source": [
    "topico = lda_top[1][1]\n",
    "topico.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "palavra = list(zip(*topico))[0]\n",
    "score = list(zip(*topico))[1]\n",
    "\n",
    "px.bar(x=palavra, y=score, labels={'x': 'Palavra', 'y': 'Score'}, title='Tópico 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-magazine",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_top = lda.show_topics(formatted=False)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, shared_yaxes=True, subplot_titles=('Tópico 1', 'Tópico 2'))\n",
    "\n",
    "for i in range(2):\n",
    "    topico = lda_top[i][1]\n",
    "    topico.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    palavra = list(zip(*topico))[0]\n",
    "    score = list(zip(*topico))[1]\n",
    "    \n",
    "    fig.add_trace(go.Bar(x=palavra, y=score,\n",
    "                         marker=dict(color=score, coloraxis=\"coloraxis\")), 1, i+1)\n",
    "    \n",
    "fig.update_layout(coloraxis=dict(colorscale='Bluered_r'), showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-bristol",
   "metadata": {},
   "source": [
    "Verificando se é **stopword**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-acquisition",
   "metadata": {},
   "outputs": [],
   "source": [
    "'que' in stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-feedback",
   "metadata": {},
   "source": [
    "Com quais tópicos nossas frases originais mais se assemelham?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-wesley",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_res = lda[corpus]\n",
    "\n",
    "for doc, as_text in zip(lda_res, dfex['Texto']):\n",
    "    print(doc, as_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-victoria",
   "metadata": {},
   "source": [
    "### Testando para novas frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-kuwait",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = ['meu cão sabe correr',\n",
    "       'sol quente e praia, quero demais']\n",
    "\n",
    "token_frase = [word_tokenize(frase) for frase in doc]\n",
    "\n",
    "corpus_ex = [dic.doc2bow(lista) for lista in token_frase]\n",
    "lda_res = lda[corpus_ex]\n",
    "\n",
    "for doc, as_text in zip(lda_res, doc):\n",
    "    print(doc, as_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-warner",
   "metadata": {},
   "source": [
    "# Exercícios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-worse",
   "metadata": {},
   "source": [
    "**Exercício 1** Crie uma função de pré-processamento de textos. A função recebe uma string e:\n",
    "- Remove caracteres numéricos\n",
    "- Remove acentuação\n",
    "- Transforma tudo para minúsculo\n",
    "- Remove stopwords\n",
    "- Retorna a `lista de palavras` contidas na string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-swaziland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_texto(txt):\n",
    "    # Faça aqui\n",
    "    return #Adicione aqui a lista a ser retornada\n",
    "\n",
    "# Resultado esperado ['hey', 'vamos', 'aprender', 'nlp', 'estudo', 'vezes', 'semana']\n",
    "preproc_texto('Hey, vamos aprender NLP??? Eu estudo 3 vezes na semana.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-interview",
   "metadata": {},
   "source": [
    "**Exercício 2** Aplique LDA à base `noticias_1.xlsx` para identificar os tópicos que emergem a partir da análise do **`Título`** das notícias. Você mesmo pode atribuir um número de tópicos e quantidade de palavras na análise (teste diferentes valores).\n",
    "\n",
    "OBS: Faça todo o pré-processamento, tokenização, aplicação do LDA, etc. conforme visto no handout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-property",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-hometown",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-humanity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ec9d19d",
   "metadata": {},
   "source": [
    "**Exercício 3** Repita o exercício anterior com outras bases. Você pode utilizar os arquivos anexos no Blackboard ou fazer o download utilizando o notebook de scraping da IstoÉ (aulas anteriores).\n",
    "\n",
    "Teste diferentes quantidades de tópicos e palavras na análise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869a6dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-telescope",
   "metadata": {},
   "source": [
    "**Exercício 4** Altere a função de limpeza. Adicione palavras para serem desconsideradas, além das contidas na lista de stop words utilizada. Você consegue perceber alterações nos resultados?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-reading",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "challenging-livestock",
   "metadata": {},
   "source": [
    "**Exercício 5** Plote um gráfico de barras para cada tópico. Cada gráfico deve conter as palavras (e scores) pertencentes ao tópicos identificados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-picking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "prospective-major",
   "metadata": {},
   "source": [
    "**Exercício 6** Adicione um Stemmer à sua etapa de pré-processamento. Aplique aos dados de notícia e avalie os resultados.\n",
    "\n",
    "https://pt.wikipedia.org/wiki/Stemização\n",
    "\n",
    "https://www.kite.com/python/docs/nltk.SnowballStemmer\n",
    "\n",
    "https://snowballstem.org/algorithms/portuguese/stemmer.html\n",
    "\n",
    "Dicas:\n",
    "\n",
    "Cria o stemmer\n",
    "```python\n",
    "stemmer = SnowballStemmer(\"portuguese\")\n",
    "```\n",
    "\n",
    "Aplica em uma lista de palavras\n",
    "\n",
    "```python\n",
    "lista_stem = [stemmer.stem(word) for word in lista_palavras]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-registration",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
