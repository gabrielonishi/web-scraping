{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "comprehensive-turkey",
   "metadata": {},
   "source": [
    "# Análise de texto de fontes desestruturadas e Web\n",
    "\n",
    "## Aula 13\n",
    "\n",
    "Nesta aula continuaremos e explorar **Análise de Tópicos** - **topic modeling** para verificar padrões (tópicos) que emergem de forma natural nos dados.\n",
    "\n",
    "A biblioteca utilizada será a **Gensim**. Para conhecer mais sobre ela, acesse https://radimrehurek.com/gensim/index.html e https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html\n",
    "\n",
    "Além disso, utilizaremos o **spacy** para fazer o reconhecimento de entidades nomeadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c5c5b3",
   "metadata": {},
   "source": [
    "## Baixar os dados no Colab\n",
    "\n",
    "Para baixar os dados no colab, utilize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480412e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://atd-insper.s3.us-east-2.amazonaws.com/aula13/noticias_base.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-cable",
   "metadata": {},
   "source": [
    "## Instalando as bibliotecas necessárias\n",
    "\n",
    "Agora, vamos importar as bibliotecas necessárias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-letter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remova o comentário caso o import falhe\n",
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remova o comentário caso o import falhe\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2602a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para reconhecimento de entidades nomeadas\n",
    "!pip install -U pip\n",
    "!pip install -U spacy\n",
    "!python -m spacy download pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e4fa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para NMF\n",
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-patrol",
   "metadata": {},
   "source": [
    "## Importando as bibliotecas necessárias\n",
    "\n",
    "Agora, vamos importar as bibliotecas necessárias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-france",
   "metadata": {},
   "outputs": [],
   "source": [
    "# para trabalhar com diretórios / sistema operacional\n",
    "import os\n",
    "\n",
    "# para trabalhar com expressões regulares\n",
    "import re\n",
    "\n",
    "# utilizada para nos indicar o caminho do executável do Python\n",
    "import sys\n",
    "\n",
    "# para pandas DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "# topic Modeling e NER\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim.models import Nmf\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# plot\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# Exemplo NMF\n",
    "from sklearn.decomposition import NMF\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-regard",
   "metadata": {},
   "source": [
    "Caso obtenha algum erro, utilize o **!pip install** para instalar a biblioteca ausente!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-voluntary",
   "metadata": {},
   "source": [
    "Você pode conferir de onde está executando o Python e qual a versão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-secondary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Executável:')\n",
    "print(sys.executable)\n",
    "\n",
    "print('\\nVersão do Python:')\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-premium",
   "metadata": {},
   "source": [
    "Vamos conferir em qual diretório iremos trabalhar (é o diretório do notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-quantity",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('O seu notebook está na pasta:')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-former",
   "metadata": {},
   "source": [
    "Versão das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-chance",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('gensim {}'.format(gensim.__version__))\n",
    "print('nltk {}'.format(nltk.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-puzzle",
   "metadata": {},
   "source": [
    "# Exemplo\n",
    "\n",
    "Vamos aprender como realizar modelagem de tópicos com o Python. Para isso, vamos criar um DataFrame de exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-caution",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfex = pd.DataFrame({'Texto': ['ações de mineradoras começam a despertar interesse dos investidores',\n",
    "                             'cresce investimento nas mineradoras, entre elas a vale devido aos dividendos',\n",
    "                             'ações de mineradoras sofrem com o lockdown na china devido ao covid',\n",
    "                             'china cresce menos que o esperado devido ao mercado de tecnologia',\n",
    "                             'sobram vagas e faltam candidatos no mercado de tecnologia',\n",
    "                             'a covid continuará por muito tempo presente, precisamos nos vacinar',\n",
    "                             'o mercado de tecnologia continua aquecido, com crescimento projetado de 13%']})\n",
    "dfex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-clark",
   "metadata": {},
   "source": [
    "Vamos considerar que cada linha deste DataFrame representa um **documento**. Ou seja, temos seis documentos nesta base de dados.\n",
    "\n",
    "Perceba que o DataFrame possui apenas uma coluna de textos, ou seja, não nenhuma variável que indique categorias. Apesar disso, podemos facilmente perceber que alguns documentos falam de assuntos específicos.\n",
    "\n",
    "Apesar de ser fácil para um humano obter esta percepção, sabemos que isto é impraticável na escala de dados produzida pelas organizações. Então, vamos precisar de técnicas de Data Science e do Python!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-symphony",
   "metadata": {},
   "source": [
    "# Pré-processamento\n",
    "\n",
    "Utilizaremos diversas técnicas aprendidas durante o curso para limpar e transformar os textos que queremos analisar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-broadcasting",
   "metadata": {},
   "source": [
    "Vamos converter o texto para **minúsculo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-point",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfex['Texto'] = dfex['Texto'].str.lower()\n",
    "\n",
    "dfex['Texto'] = dfex['Texto'].apply(remove_stopwords, stopwords='portuguese')\n",
    "\n",
    "dfex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfef3cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-recorder",
   "metadata": {},
   "source": [
    "E separar o texto em uma **lista de palavras** (tokenização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-latest",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_texto = dfex['Texto'].apply(word_tokenize)\n",
    "token_texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-modification",
   "metadata": {},
   "source": [
    "Em seguida, vamos criar um **corpus** textual, que conterá uma representação dos termos ou palavras presentes em cada documento. Aqui, teremos uma representação **Bag of Words**, semelhante ao visto nas duas últimas aulas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-pencil",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = corpora.Dictionary(token_texto)\n",
    "corpus = [dic.doc2bow(lista) for lista in token_texto]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6fa35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conferindo a vetorização de uma frase qualquer\n",
    "dic.doc2bow(\"vale hoje: a mineradora vale está em alta com os investidores\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5911678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entendendo os IDs\n",
    "dic.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf126faf",
   "metadata": {},
   "source": [
    "## Non-Negative Matrix Factorization\n",
    "\n",
    "O NMF (*Non-Negative Matrix Factorization*) irá decompor uma matriz de documentos e palavras (a matriz da vetorização *bag-of-words*) em duas outras matrizes: uma de palavras e tópicos e outra de tópicos e documentos.\n",
    "\n",
    "Veja uma imagem ilustrativa:\n",
    "\n",
    "<img src=\"https://atd-insper.s3.us-east-2.amazonaws.com/aula13/img/nmf.png\">\n",
    "\n",
    "Assim, podemos saber com quais tópicos cada documento está associado e quais palavras compõem cada tópico.\n",
    "\n",
    "Obs: o produto das matrizes de variáveis e componentes será uma aproximação da matriz original de documentos e palavras (BoW).\n",
    "\n",
    "Veja um exemplo, onde a matriz `X` é decomposta nas matrizes `W` e `H`, considerando `4` componentes:\n",
    "\n",
    "Dica: geralmente é utilizado um número de componentes menor que o tamanho da matriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3782b5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 2], \n",
    "              [3, 4]])\n",
    "\n",
    "model = NMF(n_components=4, init='random', random_state=0)\n",
    "W = model.fit_transform(X)\n",
    "H = model.components_\n",
    "\n",
    "print(f'Matriz W:\\n{W}\\n')\n",
    "print(f'Matriz H:\\n{H}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07897a2",
   "metadata": {},
   "source": [
    "Conferindo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f588e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(W, H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f16cd45",
   "metadata": {},
   "source": [
    "### NMF utilizando `gensim`\n",
    "\n",
    "Com o uso da biblioteca `gensim`, vamos criar uma instância de `Nmf` parametrizando a quantidade de tópicos e o dicionário para tradução das palavras.\n",
    "\n",
    "Veja mais detalhes da documentação em https://radimrehurek.com/gensim/models/nmf.html\n",
    "\n",
    "Vamos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32ffa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = Nmf(corpus, num_topics=2, id2word=dic)\n",
    "\n",
    "lista_topico = nmf.print_topics(num_words=3)\n",
    "\n",
    "for topico in lista_topico:\n",
    "    print(topico)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33278e0e",
   "metadata": {},
   "source": [
    "Teste com quantidade diferentes de tópicos e analise os resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b69a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = Nmf(corpus, num_topics=3, id2word=dic)\n",
    "\n",
    "lista_topico = nmf.print_topics(num_words=3)\n",
    "\n",
    "for topico in lista_topico:\n",
    "    print(topico)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-proxy",
   "metadata": {},
   "source": [
    "## Plot dos tópicos encontrados\n",
    "\n",
    "É conveninente ter a possibilidade de visualizar graficamente os tópicos encontrados.\n",
    "\n",
    "Para isso, utilizaremos uma biblioteca chamada **plotly**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-technical",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_top = nmf.show_topics(formatted=False)\n",
    "\n",
    "topico = nmf_top[0][1]\n",
    "topico.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "palavra = list(zip(*topico))[0]\n",
    "score = list(zip(*topico))[1]\n",
    "\n",
    "px.bar(x=palavra, y=score, labels={'x': 'Palavra', 'y': 'Score'}, title='Tópico 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-coordinate",
   "metadata": {},
   "outputs": [],
   "source": [
    "topico = nmf_top[1][1]\n",
    "topico.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "palavra = list(zip(*topico))[0]\n",
    "score = list(zip(*topico))[1]\n",
    "\n",
    "px.bar(x=palavra, y=score, labels={'x': 'Palavra', 'y': 'Score'}, title='Tópico 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-magazine",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_top = nmf.show_topics(formatted=False)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, shared_yaxes=True, subplot_titles=('Tópico 1', 'Tópico 2'))\n",
    "\n",
    "for i in range(2):\n",
    "    topico = nmf_top[i][1]\n",
    "    topico.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    palavra = list(zip(*topico))[0]\n",
    "    score = list(zip(*topico))[1]\n",
    "    \n",
    "    fig.add_trace(go.Bar(x=palavra, y=score,\n",
    "                         marker=dict(color=score, coloraxis=\"coloraxis\")), 1, i+1)\n",
    "    \n",
    "fig.update_layout(coloraxis=dict(colorscale='Bluered_r'), showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-feedback",
   "metadata": {},
   "source": [
    "## Composição dos documentos\n",
    "\n",
    "Um documento pode ter frases ou partes que falam de diversos assuntos, assim, será composto por um ou mais tópicos. Vamos analisar com quais tópicos um documento mais se relaciona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-wesley",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_res = nmf[corpus]\n",
    "\n",
    "for doc, as_text in zip(nmf_res, dfex['Texto']):\n",
    "    print(doc, as_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-victoria",
   "metadata": {},
   "source": [
    "### Testando para novas frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-kuwait",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = ['ações da ptr4 desabam devido a interferências do governo',\n",
    "       'empresas que não investirem em dados e tecnologia terão um futuro duro pela frente',\n",
    "       'mineradoras também precisam investir em tecnologia, elas sofrem mas investem']\n",
    "\n",
    "token_frase = [word_tokenize(frase) for frase in doc]\n",
    "\n",
    "corpus_ex = [dic.doc2bow(lista) for lista in token_frase]\n",
    "nmf_res = nmf[corpus_ex]\n",
    "\n",
    "for doc, as_text in zip(nmf_res, doc):\n",
    "    print(doc, as_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960aa0f4",
   "metadata": {},
   "source": [
    "## NER - Named Entity Recognition\n",
    "\n",
    "É uma necessidade comum entender sobre o que certo documento está falando. Nas aulas anteriores, vimos que podemos utilizar expressões regulares para identificar certas palavras ou padrões de nosso interesse. Assim, poderíamos por exemplo filtrar todos os ducumentos que mencionam `PTR4` ou demais palavra associadas a Petrobras.\n",
    "\n",
    "Com o uso de técnicas de Machine Learning supervisionadas, conseguimos treinar um modelo que consegue predizer a categoria de uma notícia. Além disso, com o uso de *topic modeling*, podemos identificar padrões que emergem naturalmente dos textos, podendo interpretar estes padrões uma vez que sabemos quais palavras compõem o tópico e com quais tópicos o documento mais se associa.\n",
    "\n",
    "Agora, iremos apresentar uma alternativa: detecção de entidades nomeadas. Uma entidade nomeada é basicamente um objeto que possui identificação adequada e pode ser denotado com um nome próprio. Entidades Nomeadas podem ser um lugar, pessoa, organização, objeto, entidade geográfica, etc.\n",
    "\n",
    "Do ponto de vista técnico, podemos treinar nosso próprio modelo para NER, ou utilizar modelos pré-treinados. Estes modelos já incorporam conhecimento sobre entidades comuns em determinada lingua (português, inglês) ou cenário (economia, tecnologia).\n",
    "\n",
    "Vamos a um exemplo. Primeiro, abrimos o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1a0547",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ed7a2f",
   "metadata": {},
   "source": [
    "E criamos um texto qualquer com uma notícia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46147f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'Sou de São Paulo, no Brasil e nasci em 1988. As ações PTR4 não param de cair na Bovespa.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d9de6d",
   "metadata": {},
   "source": [
    "Vamos transformar a string em um documento `spacy`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d049d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = model(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0de059f",
   "metadata": {},
   "source": [
    "e imprimir as entidades nomeadas identificadas no texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd88bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([(entity, entity.label_) for entity in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3143fb0e",
   "metadata": {},
   "source": [
    "Veja um outro exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3046cf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "noticia = 'Depois de anos de fracassos e adiamentos, a companhia aeronáutica americana Boeing tentará voltar à concorrência com a SpaceX para servir de “táxi” espacial para a Nasa, na Flórida'\n",
    "doc_noticia = model(noticia)\n",
    "print([(entity, entity.label_) for entity in doc_noticia.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebd79f1",
   "metadata": {},
   "source": [
    "Podemos pedir ao `spacy` que explique o que quer dizer as siglas utilizadas para identificar as entidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce25d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('LOC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d986d734",
   "metadata": {},
   "source": [
    "Veja uma forma diferente de visualizar as entidades, com o uso de um laço `for`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b754561",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in doc_noticia.ents:\n",
    "    print(word.text,word.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bd7b8e",
   "metadata": {},
   "source": [
    "Podemos também exibir graficamente as entidades identificadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b472b9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ae5bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc_noticia, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f35ed1",
   "metadata": {},
   "source": [
    "Um exemplo mais completo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f117f589",
   "metadata": {},
   "outputs": [],
   "source": [
    "descricao = '''SÃO PAULO (Reuters) – O principal índice da bolsa brasileira subiu nesta quinta-feira, impulsionado por ações de siderúrgicas e mineradoras, destoando do recuo em Wall Street, que teve outro dia volátil.\n",
    "\n",
    "Vale cresceu na esteira dos preços do minério de ferro e CSN disparou com anúncio de recompra de ações. A Eletrobras também foi destaque positivo, após aval do Tribunal de Contas da União (TCU) para privatização. WEG e Hapvida foram destaques de queda.\n",
    "\n",
    "\n",
    "O Ibovespa subiu 0,71%, a 107.005,22 pontos. O volume financeiro da sessão foi de 24,7 bilhões de reais.\n",
    "\n",
    "“O mercado ficou bem pressionado na véspera e teve um pouco de recuperação em função da alta das commodities”, disse Luiz Roberto Monteiro, operador da mesa institucional da Renascença. Ele também citou recentes elevações das projeções de crescimento econômico para o Brasil neste ano.\n",
    "\n",
    "A XP dobrou a estimativa e agora espera expansão de 1,6% do Produto Interno Bruto (PIB)do país em 2022, embora tenha elevado também a expectativa de inflação para até 2023. E o ministério da Economia manteve sua projeção para o PIB em 1,5%, mas espera uma inflação mais alta. '''\n",
    "\n",
    "print(descricao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2433fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = model(descricao)\n",
    "\n",
    "print([(entity, entity.label_) for entity in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304bd29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-warner",
   "metadata": {},
   "source": [
    "# Exercícios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-worse",
   "metadata": {},
   "source": [
    "**Exercício 1** Crie uma função de pré-processamento de textos. A função recebe uma string e:\n",
    "- Remove caracteres numéricos\n",
    "- Remove acentuação\n",
    "- Transforma tudo para minúsculo\n",
    "- Remove stopwords\n",
    "- Retorna a lista de palavras contidas na string\n",
    "\n",
    "Você pode reaproveitar a função feita na aula passada!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-swaziland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_texto(txt):\n",
    "    \n",
    "    return txt_limpo\n",
    "\n",
    "# Resultado esperado ['hey', 'vamos', 'aprender', 'nlp', 'estudo', 'vezes', 'semana']\n",
    "preproc_texto('Hey, vamos aprender NLP??? Eu estudo 3 vezes na semana.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbadd862",
   "metadata": {},
   "source": [
    "**Exercício 2** Crie uma função que recebe um PandasSeries onde cada linha representa um texto.\n",
    "Sua função deve aplicar a função de `preproc_texto` em cada linha, devolvendo um PandasSeries onde cada linha contem uma lista de tokens (conforme exercício anterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3682940a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "amended-interview",
   "metadata": {},
   "source": [
    "**Exercício 3** Aplique **NMF** à base `noticias_base.csv` para identificar os tópicos que emergem a partir da análise do **`Título`** das notícias. Você mesmo pode atribuir um número de tópicos e quantidade de palavras na análise (teste diferentes valores).\n",
    "\n",
    "OBS: Faça todo o pré-processamento, tokenização, aplicação do `NMF`, etc. conforme visto no handout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-property",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-hometown",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-humanity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4db1256",
   "metadata": {},
   "source": [
    "**Exercício 4** Crie uma função que recebe um texto e devolve as entidades nomeadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dff8b16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-telescope",
   "metadata": {},
   "source": [
    "**Exercício 5** Crie uma função que recebe um PandasSeries onde cada linha representa um texto. Reconheça e devolva as entidades nomeadas com o uso da função feita no exercício anterior (aplique em cada linha).\n",
    "\n",
    "Teste com a base `noticias_1.xlsx` considerando a coluna **`Descrição`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-reading",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-registration",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
